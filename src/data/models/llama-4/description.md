Llama 4 is the latest in Meta AI's open-source Llama family of language models, released in April 2025. It marks a significant evolution by introducing a mixture-of-experts multimodal architecture. Llama 4 comes in two main variants: Llama 4 Scout (a smaller, high-speed model geared toward efficiency) and Llama 4 Maverick (a larger model with Mixture-of-Experts design featuring 17B active parameters with 128 experts, totaling ~2 trillion parameters). Both variants are multimodal â€“ they can accept text and images as input and output text. Llama 4 can "see" images, allowing it to describe pictures or diagrams and incorporate visual context into its reasoning. The architecture change to MoE means that Llama 4 dynamically routes different parts of a task to different subsets of the model, improving performance without inflating inference cost. Only ~10-17B parameters are used per token in Maverick's case, meaning it retains agility despite the huge total parameter count. Meta trained Llama 4 on a massive dataset (likely several times larger than Llama 2's, including diverse web data, code, and images for the multimodal part). They also applied extensive fine-tuning for helpfulness and safety, addressing criticisms of Llama 3. Meta claimed Llama 4 achieved state-of-the-art or near-SOTA on many benchmarks, even besting OpenAI's GPT-4o on certain leaderboards. Key specs include context length support up to 10 million tokens thanks to an external memory mechanism, reinforcement learning from human feedback (RLHF), and chain-of-thought reasoning for complex queries. Llama 4 is released under a community license (source-available), meaning it's free for research and commercial use with some restrictions. It significantly improved over Llama 3 with enhanced multilingual support (8 languages), better reasoning due to MoE, multimodality, stability, and customizable personas. In summary, Llama 4 is an open, cutting-edge LLM that rivals top closed models, bringing together huge scale (trillion+ params MoE), long context, image understanding, and open availability.
